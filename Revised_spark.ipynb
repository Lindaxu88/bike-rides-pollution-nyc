{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"n7Nm7yGNZdh7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741760852834,"user_tz":420,"elapsed":22088,"user":{"displayName":"YEMAN XU","userId":"04669461631257758688"}},"outputId":"fee48153-5336-442f-a3ed-698b6bbea0d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/MyDrive/405 Final Project\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LpqdCXWsZoXI","executionInfo":{"status":"ok","timestamp":1741760853828,"user_tz":420,"elapsed":10,"user":{"displayName":"YEMAN XU","userId":"04669461631257758688"}},"outputId":"daccd351-19c2-4ed5-81da-6c7faad80d41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/405 Final Project'\n","/content\n"]}]},{"cell_type":"code","source":["!ls -l \"/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KD3ZTya7aMHb","executionInfo":{"status":"ok","timestamp":1741760854664,"user_tz":420,"elapsed":107,"user":{"displayName":"YEMAN XU","userId":"04669461631257758688"}},"outputId":"4d08de84-887f-4b7b-a4a4-d20f5b30c065"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n","/bin/bash: -c: line 2: syntax error: unexpected end of file\n"]}]},{"cell_type":"code","source":["%cd \"/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5-pOZwR9aTUt","executionInfo":{"status":"ok","timestamp":1741760855348,"user_tz":420,"elapsed":77,"user":{"displayName":"YEMAN XU","userId":"04669461631257758688"}},"outputId":"89d24ce0-2844-40d9-d1c6-cae0b50882b5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project\n"]}]},{"cell_type":"code","source":["!spark-submit Revised_spark.py \\\n","    \"2017_citibike_morning_rush_07_09.csv,2018_citibike_morning_hours_07_09.csv,2018_citibike_rush_hours_15_20.csv,2019_citibike_morning_rush_07_09.csv\" \\\n","    \"Air_Quality.csv\" \\\n","    \"joined_output\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OEwlahAtaVc9","executionInfo":{"status":"ok","timestamp":1741760897125,"user_tz":420,"elapsed":41187,"user":{"displayName":"YEMAN XU","userId":"04669461631257758688"}},"outputId":"d77eee53-38e8-4cd4-c606-c88eeb455f65"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["25/03/12 06:27:49 INFO SparkContext: Running Spark version 3.5.5\n","25/03/12 06:27:49 INFO SparkContext: OS info Linux, 6.1.85+, amd64\n","25/03/12 06:27:49 INFO SparkContext: Java version 11.0.26\n","25/03/12 06:27:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","25/03/12 06:27:49 INFO ResourceUtils: ==============================================================\n","25/03/12 06:27:49 INFO ResourceUtils: No custom resources configured for spark.driver.\n","25/03/12 06:27:49 INFO ResourceUtils: ==============================================================\n","25/03/12 06:27:49 INFO SparkContext: Submitted application: Revised_spark.py\n","25/03/12 06:27:50 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n","25/03/12 06:27:50 INFO ResourceProfile: Limiting resource is cpu\n","25/03/12 06:27:50 INFO ResourceProfileManager: Added ResourceProfile id: 0\n","25/03/12 06:27:50 INFO SecurityManager: Changing view acls to: root\n","25/03/12 06:27:50 INFO SecurityManager: Changing modify acls to: root\n","25/03/12 06:27:50 INFO SecurityManager: Changing view acls groups to: \n","25/03/12 06:27:50 INFO SecurityManager: Changing modify acls groups to: \n","25/03/12 06:27:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\n","25/03/12 06:27:51 INFO Utils: Successfully started service 'sparkDriver' on port 34513.\n","25/03/12 06:27:51 INFO SparkEnv: Registering MapOutputTracker\n","25/03/12 06:27:51 INFO SparkEnv: Registering BlockManagerMaster\n","25/03/12 06:27:51 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n","25/03/12 06:27:51 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n","25/03/12 06:27:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","25/03/12 06:27:52 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-0c237b31-daf7-4bb3-8e48-aa3566fd7b54\n","25/03/12 06:27:52 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB\n","25/03/12 06:27:52 INFO SparkEnv: Registering OutputCommitCoordinator\n","25/03/12 06:27:53 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n","25/03/12 06:27:53 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n","25/03/12 06:27:54 INFO Executor: Starting executor ID driver on host 4f1431d68959\n","25/03/12 06:27:54 INFO Executor: OS info Linux, 6.1.85+, amd64\n","25/03/12 06:27:54 INFO Executor: Java version 11.0.26\n","25/03/12 06:27:54 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n","25/03/12 06:27:54 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@1bbc7d84 for default.\n","25/03/12 06:27:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44027.\n","25/03/12 06:27:54 INFO NettyBlockTransferService: Server created on 4f1431d68959:44027\n","25/03/12 06:27:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n","25/03/12 06:27:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 4f1431d68959, 44027, None)\n","25/03/12 06:27:54 INFO BlockManagerMasterEndpoint: Registering block manager 4f1431d68959:44027 with 434.4 MiB RAM, BlockManagerId(driver, 4f1431d68959, 44027, None)\n","25/03/12 06:27:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 4f1431d68959, 44027, None)\n","25/03/12 06:27:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 4f1431d68959, 44027, None)\n","25/03/12 06:27:55 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n","25/03/12 06:27:55 INFO SharedState: Warehouse path is 'file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405%20Final%20Project/spark-warehouse'.\n","25/03/12 06:27:57 INFO InMemoryFileIndex: It took 112 ms to list leaf files for 4 paths.\n","25/03/12 06:27:57 INFO InMemoryFileIndex: It took 27 ms to list leaf files for 4 paths.\n","25/03/12 06:28:02 INFO FileSourceStrategy: Pushed Filters: \n","25/03/12 06:28:02 INFO FileSourceStrategy: Post-Scan Filters: (length(trim(value#0, None)) > 0)\n","25/03/12 06:28:04 INFO CodeGenerator: Code generated in 1142.032776 ms\n","25/03/12 06:28:04 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 199.9 KiB, free 434.2 MiB)\n","25/03/12 06:28:05 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n","25/03/12 06:28:05 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 4f1431d68959:44027 (size: 34.5 KiB, free: 434.4 MiB)\n","25/03/12 06:28:05 INFO SparkContext: Created broadcast 0 from csv at NativeMethodAccessorImpl.java:0\n","25/03/12 06:28:05 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n","25/03/12 06:28:05 INFO SparkContext: Starting job: csv at NativeMethodAccessorImpl.java:0\n","25/03/12 06:28:05 INFO DAGScheduler: Got job 0 (csv at NativeMethodAccessorImpl.java:0) with 1 output partitions\n","25/03/12 06:28:05 INFO DAGScheduler: Final stage: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0)\n","25/03/12 06:28:05 INFO DAGScheduler: Parents of final stage: List()\n","25/03/12 06:28:06 INFO DAGScheduler: Missing parents: List()\n","25/03/12 06:28:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0), which has no missing parents\n","25/03/12 06:28:06 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.5 KiB, free 434.2 MiB)\n","25/03/12 06:28:06 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.4 KiB, free 434.2 MiB)\n","25/03/12 06:28:06 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 4f1431d68959:44027 (size: 6.4 KiB, free: 434.4 MiB)\n","25/03/12 06:28:06 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1585\n","25/03/12 06:28:06 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n","25/03/12 06:28:06 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n","25/03/12 06:28:06 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (4f1431d68959, executor driver, partition 0, PROCESS_LOCAL, 9700 bytes) \n","25/03/12 06:28:06 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n","25/03/12 06:28:07 INFO CodeGenerator: Code generated in 95.812939 ms\n","25/03/12 06:28:07 INFO FileScanRDD: Reading File path: file:///content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405%20Final%20Project/2017_citibike_morning_rush_07_09.csv, range: 0-134217728, partition values: [empty row]\n","25/03/12 06:28:07 INFO CodeGenerator: Code generated in 139.056677 ms\n","25/03/12 06:28:08 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1680 bytes result sent to driver\n","25/03/12 06:28:08 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1670 ms on 4f1431d68959 (executor driver) (1/1)\n","25/03/12 06:28:08 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n","25/03/12 06:28:08 INFO DAGScheduler: ResultStage 0 (csv at NativeMethodAccessorImpl.java:0) finished in 2.359 s\n","25/03/12 06:28:08 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n","25/03/12 06:28:08 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n","25/03/12 06:28:08 INFO DAGScheduler: Job 0 finished: csv at NativeMethodAccessorImpl.java:0, took 2.640718 s\n","25/03/12 06:28:08 INFO CodeGenerator: Code generated in 49.077733 ms\n","25/03/12 06:28:08 INFO FileSourceStrategy: Pushed Filters: \n","25/03/12 06:28:08 INFO FileSourceStrategy: Post-Scan Filters: \n","25/03/12 06:28:09 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 199.9 KiB, free 434.0 MiB)\n","25/03/12 06:28:09 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 433.9 MiB)\n","25/03/12 06:28:09 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 4f1431d68959:44027 (size: 34.5 KiB, free: 434.3 MiB)\n","25/03/12 06:28:09 INFO SparkContext: Created broadcast 2 from csv at NativeMethodAccessorImpl.java:0\n","25/03/12 06:28:09 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n","25/03/12 06:28:11 INFO FileSourceStrategy: Pushed Filters: \n","25/03/12 06:28:11 INFO FileSourceStrategy: Post-Scan Filters: \n","25/03/12 06:28:11 INFO ParquetUtils: Using default output committer for Parquet: org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","25/03/12 06:28:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","25/03/12 06:28:11 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:11 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","25/03/12 06:28:11 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","25/03/12 06:28:11 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:11 ERROR FileOutputCommitter: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0\n","25/03/12 06:28:12 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 4f1431d68959:44027 in memory (size: 34.5 KiB, free: 434.4 MiB)\n","25/03/12 06:28:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 4f1431d68959:44027 in memory (size: 34.5 KiB, free: 434.4 MiB)\n","25/03/12 06:28:12 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 4f1431d68959:44027 in memory (size: 6.4 KiB, free: 434.4 MiB)\n","25/03/12 06:28:12 INFO CodeGenerator: Code generated in 527.807065 ms\n","25/03/12 06:28:12 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 199.7 KiB, free 434.2 MiB)\n","25/03/12 06:28:12 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 34.5 KiB, free 434.2 MiB)\n","25/03/12 06:28:12 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 4f1431d68959:44027 (size: 34.5 KiB, free: 434.4 MiB)\n","25/03/12 06:28:13 INFO SparkContext: Created broadcast 3 from parquet at NativeMethodAccessorImpl.java:0\n","25/03/12 06:28:13 INFO FileSourceScanExec: Planning scan with bin packing, max size: 134217728 bytes, open cost is considered as scanning 4194304 bytes.\n","25/03/12 06:28:13 INFO SparkContext: Starting job: parquet at NativeMethodAccessorImpl.java:0\n","25/03/12 06:28:13 INFO DAGScheduler: Got job 1 (parquet at NativeMethodAccessorImpl.java:0) with 9 output partitions\n","25/03/12 06:28:13 INFO DAGScheduler: Final stage: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0)\n","25/03/12 06:28:13 INFO DAGScheduler: Parents of final stage: List()\n","25/03/12 06:28:13 INFO DAGScheduler: Missing parents: List()\n","25/03/12 06:28:13 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n","25/03/12 06:28:13 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 239.1 KiB, free 433.9 MiB)\n","25/03/12 06:28:13 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 84.2 KiB, free 433.9 MiB)\n","25/03/12 06:28:13 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 4f1431d68959:44027 (size: 84.2 KiB, free: 434.3 MiB)\n","25/03/12 06:28:13 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n","25/03/12 06:28:13 INFO DAGScheduler: Submitting 9 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8))\n","25/03/12 06:28:13 INFO TaskSchedulerImpl: Adding task set 1.0 with 9 tasks resource profile 0\n","25/03/12 06:28:13 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (4f1431d68959, executor driver, partition 0, PROCESS_LOCAL, 9700 bytes) \n","25/03/12 06:28:13 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 2) (4f1431d68959, executor driver, partition 1, PROCESS_LOCAL, 9701 bytes) \n","25/03/12 06:28:13 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n","25/03/12 06:28:13 INFO Executor: Running task 1.0 in stage 1.0 (TID 2)\n","25/03/12 06:28:14 INFO CodeGenerator: Code generated in 409.385901 ms\n","25/03/12 06:28:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","25/03/12 06:28:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","25/03/12 06:28:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","25/03/12 06:28:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","25/03/12 06:28:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","25/03/12 06:28:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","25/03/12 06:28:14 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:14 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","25/03/12 06:28:14 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","25/03/12 06:28:14 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:14 INFO FileScanRDD: Reading File path: file:///content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405%20Final%20Project/2018_citibike_morning_hours_07_09.csv, range: 0-134217728, partition values: [empty row]\n","25/03/12 06:28:14 INFO CodecConfig: Compression: SNAPPY\n","25/03/12 06:28:14 INFO CodecConfig: Compression: SNAPPY\n","25/03/12 06:28:14 INFO CodeGenerator: Code generated in 126.974679 ms\n","25/03/12 06:28:14 INFO ParquetOutputFormat: ParquetRecordWriter [block size: 134217728b, row group padding size: 8388608b, validating: false]\n","25/03/12 06:28:14 INFO ParquetWriteSupport: Initialized Parquet WriteSupport with Catalyst schema:\n","{\n","  \"type\" : \"struct\",\n","  \"fields\" : [ {\n","    \"name\" : \"trip_duration\",\n","    \"type\" : \"string\",\n","    \"nullable\" : true,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"trip_start_time\",\n","    \"type\" : \"string\",\n","    \"nullable\" : true,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"start_station_id\",\n","    \"type\" : \"string\",\n","    \"nullable\" : true,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"start_station_latitude\",\n","    \"type\" : \"string\",\n","    \"nullable\" : true,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"start_station_longitude\",\n","    \"type\" : \"string\",\n","    \"nullable\" : true,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"end_station_id\",\n","    \"type\" : \"string\",\n","    \"nullable\" : true,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"user_type\",\n","    \"type\" : \"string\",\n","    \"nullable\" : true,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"borough\",\n","    \"type\" : \"string\",\n","    \"nullable\" : false,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"zone\",\n","    \"type\" : \"string\",\n","    \"nullable\" : false,\n","    \"metadata\" : { }\n","  }, {\n","    \"name\" : \"trip_start_timestamp\",\n","    \"type\" : \"timestamp\",\n","    \"nullable\" : true,\n","    \"metadata\" : { }\n","  } ]\n","}\n","and corresponding Parquet message type:\n","message spark_schema {\n","  optional binary trip_duration (STRING);\n","  optional binary trip_start_time (STRING);\n","  optional binary start_station_id (STRING);\n","  optional binary start_station_latitude (STRING);\n","  optional binary start_station_longitude (STRING);\n","  optional binary end_station_id (STRING);\n","  optional binary user_type (STRING);\n","  required binary borough (STRING);\n","  required binary zone (STRING);\n","  optional int96 trip_start_timestamp;\n","}\n","\n","       \n","25/03/12 06:28:14 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)\n","java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","25/03/12 06:28:15 INFO TaskSetManager: Starting task 2.0 in stage 1.0 (TID 3) (4f1431d68959, executor driver, partition 2, PROCESS_LOCAL, 9698 bytes) \n","25/03/12 06:28:15 INFO Executor: Running task 2.0 in stage 1.0 (TID 3)\n","25/03/12 06:28:15 WARN TaskSetManager: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","25/03/12 06:28:15 ERROR TaskSetManager: Task 0 in stage 1.0 failed 1 times; aborting job\n","25/03/12 06:28:15 INFO TaskSchedulerImpl: Cancelling stage 1\n","25/03/12 06:28:15 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","25/03/12 06:28:15 INFO Executor: Executor is trying to kill task 1.0 in stage 1.0 (TID 2), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","25/03/12 06:28:15 INFO Executor: Executor is trying to kill task 2.0 in stage 1.0 (TID 3), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","25/03/12 06:28:15 INFO TaskSchedulerImpl: Stage 1 was cancelled\n","25/03/12 06:28:15 INFO DAGScheduler: ResultStage 1 (parquet at NativeMethodAccessorImpl.java:0) failed in 2.028 s due to Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","25/03/12 06:28:15 INFO DAGScheduler: Job 1 failed: parquet at NativeMethodAccessorImpl.java:0, took 2.076240 s\n","25/03/12 06:28:15 ERROR FileFormatWriter: Aborting job 1f4fb270-dde2-4423-b4b8-503ac7cab838.\n","org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n","\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n","\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n","\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n","\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n","\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n","\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\t... 1 more\n","25/03/12 06:28:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","25/03/12 06:28:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","25/03/12 06:28:15 INFO SQLHadoopMapReduceCommitProtocol: Using user defined output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:15 INFO FileOutputCommitter: File Output Committer Algorithm version is 1\n","25/03/12 06:28:15 INFO FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","25/03/12 06:28:15 INFO SQLHadoopMapReduceCommitProtocol: Using output committer class org.apache.parquet.hadoop.ParquetOutputCommitter\n","25/03/12 06:28:15 INFO Executor: Executor killed task 2.0 in stage 1.0 (TID 3), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","25/03/12 06:28:15 WARN TaskSetManager: Lost task 2.0 in stage 1.0 (TID 3) (4f1431d68959 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:)\n","25/03/12 06:28:16 INFO Executor: Executor interrupted and killed task 1.0 in stage 1.0 (TID 2), reason: Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","25/03/12 06:28:16 WARN TaskSetManager: Lost task 1.0 in stage 1.0 (TID 2) (4f1431d68959 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:)\n","25/03/12 06:28:16 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n","Traceback (most recent call last):\n","  File \"/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project/Revised_spark.py\", line 203, in <module>\n","    main()\n","  File \"/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project/Revised_spark.py\", line 110, in main\n","    .parquet(\"../output/citibike_zones\")\n","     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/readwriter.py\", line 1721, in parquet\n","  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n","  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n","  File \"/usr/local/lib/python3.11/dist-packages/pyspark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n","py4j.protocol.Py4JJavaError: An error occurred while calling o135.parquet.\n",": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (4f1431d68959 executor driver): java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","\n","Driver stacktrace:\n","\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n","\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n","\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n","\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n","\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n","\tat scala.Option.foreach(Option.scala:407)\n","\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n","\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n","\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n","\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n","\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n","\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n","\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n","\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n","\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n","\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n","\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n","\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n","\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n","\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n","\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n","\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n","\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n","\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n","\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n","\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)\n","\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)\n","\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)\n","\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n","\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:802)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n","\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n","\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n","\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n","\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n","\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n","\tat py4j.Gateway.invoke(Gateway.java:282)\n","\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n","\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n","\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n","\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n","\tat java.base/java.lang.Thread.run(Thread.java:829)\n","Caused by: java.io.IOException: Mkdirs failed to create file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/output/citibike_zones/_temporary/0/_temporary/attempt_202503120628133942302969807556523_0001_m_000000_1 (exists=false, cwd=file:/content/drive/.shortcut-targets-by-id/16vC_ElnJuNcHxf_KgoFCBDED3h1xroyh/405 Final Project)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:515)\n","\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:500)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1195)\n","\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:1175)\n","\tat org.apache.parquet.hadoop.util.HadoopOutputFile.create(HadoopOutputFile.java:74)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:347)\n","\tat org.apache.parquet.hadoop.ParquetFileWriter.<init>(ParquetFileWriter.java:314)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:484)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:422)\n","\tat org.apache.parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:411)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.<init>(ParquetOutputWriter.scala:36)\n","\tat org.apache.spark.sql.execution.datasources.parquet.ParquetUtils$$anon$1.newInstance(ParquetUtils.scala:490)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:161)\n","\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:146)\n","\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:389)\n","\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n","\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n","\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n","\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n","\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n","\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n","\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n","\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n","\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n","\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n","\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n","\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n","\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n","\t... 1 more\n","\n","25/03/12 06:28:16 INFO SparkContext: Invoking stop() from shutdown hook\n","25/03/12 06:28:16 INFO SparkContext: SparkContext is stopping with exitCode 0.\n","25/03/12 06:28:16 INFO SparkUI: Stopped Spark web UI at http://4f1431d68959:4040\n","25/03/12 06:28:16 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\n","25/03/12 06:28:16 INFO MemoryStore: MemoryStore cleared\n","25/03/12 06:28:16 INFO BlockManager: BlockManager stopped\n","25/03/12 06:28:16 INFO BlockManagerMaster: BlockManagerMaster stopped\n","25/03/12 06:28:16 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\n","25/03/12 06:28:16 INFO SparkContext: Successfully stopped SparkContext\n","25/03/12 06:28:16 INFO ShutdownHookManager: Shutdown hook called\n","25/03/12 06:28:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-d45d4bd4-21db-42b3-8119-af57593a6d6a\n","25/03/12 06:28:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-16dfbdf0-c7ce-4036-98fa-c6caada066e1\n","25/03/12 06:28:16 INFO ShutdownHookManager: Deleting directory /tmp/spark-d45d4bd4-21db-42b3-8119-af57593a6d6a/pyspark-78acde1f-a0d5-4a00-b9a4-40aff5abfc79\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":382},"id":"Ag4MN17eACT3","outputId":"7f3f3da1-ee4e-4a67-f0d1-fe2e639d88c9","executionInfo":{"status":"error","timestamp":1741676944784,"user_tz":420,"elapsed":55,"user":{"displayName":"HAORUI GAO","userId":"04940035010516353456"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING: No (or incomplete) arguments were provided.\n","Using default file paths. Adjust them as needed.\n","\n"]},{"output_type":"error","ename":"AnalysisException","evalue":"[PATH_NOT_FOUND] Path does not exist: file:/content/2017_citibike_morning_rush_07_09.csv.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-3ef02d7e8b91>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-7-3ef02d7e8b91>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recursiveFileLookup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"true\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spark.sql.files.maxPartitionBytes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"128MB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcitibike_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             .select(\n\u001b[1;32m     51\u001b[0m                 \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tripduration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trip_duration\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/2017_citibike_morning_rush_07_09.csv."]}],"source":["import sys\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import (\n","    col, to_timestamp, date_trunc, row_number, split,\n","    regexp_extract, regexp_replace, when,\n","    monotonically_increasing_id, broadcast, date_format, to_date,\n","    avg, first\n",")\n","from pyspark.sql import Window\n","\n","def main():\n","    \"\"\"\n","    This script joins Citibike rides data with Air Quality data on matching date + zone.\n","    It falls back to default CSV/paths if you don't pass arguments.\n","\n","    Usage (if providing arguments):\n","        spark-submit spark-job.py \\\n","            [citibike_paths (comma-separated)] \\\n","            [air_quality_path] \\\n","            [output_path]\n","    \"\"\"\n","    # If 3 user arguments are provided, parse them. Otherwise, use fallback defaults.\n","    if len(sys.argv) == 4:\n","        citibike_paths = sys.argv[1].split(\",\")\n","        air_quality_path = sys.argv[2]\n","        output_path = sys.argv[3]\n","    else:\n","        print(\"WARNING: No (or incomplete) arguments were provided.\")\n","        print(\"Using default file paths. Adjust them as needed.\\n\")\n","        citibike_paths = [\n","            \"2017_citibike_morning_rush_07_09.csv\",\n","            \"2018_citibike_morning_hours_07_09.csv\",\n","            \"2019_citibike_morning_rush_07_09.csv\"\n","        ]\n","        air_quality_path = \"Air_Quality.csv\"\n","        output_path = \"joined_output\"\n","\n","    # Create Spark session\n","    spark = SparkSession.builder.getOrCreate()\n","\n","    ######################################################\n","    # Load and preprocess Citibike rides data\n","    ######################################################\n","    rides = (\n","        spark.read\n","            .option(\"header\", \"true\")\n","            .option(\"recursiveFileLookup\", \"true\")\n","            .option(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n","            .csv(citibike_paths)\n","            .select(\n","                col(\"tripduration\").alias(\"trip_duration\"),\n","                col(\"starttime\").alias(\"trip_start_time\"),\n","                col(\"startstationid\").alias(\"start_station_id\"),\n","                col(\"startstationlatitude\").alias(\"start_station_latitude\"),\n","                col(\"startstationlongitude\").alias(\"start_station_longitude\"),\n","                col(\"endstationid\").alias(\"end_station_id\"),\n","                col(\"usertype\").alias(\"user_type\")\n","            )\n","    )\n","\n","    # Determine borough based on starting station coordinates.\n","    rides = rides.withColumn(\n","        \"zone\",\n","        when(\n","            (col(\"start_station_latitude\").between(40.70, 40.75)) &\n","            (col(\"start_station_longitude\").between(-74.02, -73.93)),\n","            \"lower-manhattan\"\n","        ).when(\n","            (col(\"start_station_latitude\").between(40.75, 40.82)) &\n","            (col(\"start_station_longitude\").between(-74.02, -73.93)),\n","            \"mid-manhattan\"\n","        ).when(\n","            (col(\"start_station_latitude\").between(40.82, 40.88)) &\n","            (col(\"start_station_longitude\").between(-74.02, -73.93)),\n","            \"upper-manhattan\"\n","        ).when(\n","            (col(\"start_station_latitude\").between(40.57, 40.70)) &\n","            (col(\"start_station_longitude\").between(-74.05, -73.85)),\n","            \"Brooklyn\"\n","        ).when(\n","            (col(\"start_station_latitude\").between(40.70, 40.80)) &\n","            (col(\"start_station_longitude\").between(-73.93, -73.70)),\n","            \"Queens\"\n","        ).when(\n","            (col(\"start_station_latitude\") >= 40.88) & (col(\"start_station_latitude\") < 41.00) &\n","            (col(\"start_station_longitude\").between(-73.93, -73.70)),\n","            \"Bronx\"\n","        ).otherwise(\"Unknown\")\n","    )\n","\n","    # Convert trip_start_time to timestamp for further processing.\n","    rides = rides.withColumn(\n","        \"trip_start_timestamp\",\n","        to_timestamp(col(\"trip_start_time\"), \"MM/dd/yyyy HH:mm:ss\")\n","    )\n","\n","    # -------------------------------------------------------\n","    # (REMOVED) the optional debugging write that caused errors:\n","    #\n","    # rides.write \\\n","    #     .option(\"maxRecordsPerFile\", 100000) \\\n","    #     .mode(\"overwrite\") \\\n","    #     .parquet(\"../output/citibike_zones\")\n","    # -------------------------------------------------------\n","\n","    ######################################################\n","    # Load and preprocess Air Quality data\n","    ######################################################\n","    air_quality = (\n","        spark.read\n","            .option(\"header\", \"true\")\n","            .option(\"recursiveFileLookup\", \"true\")\n","            .option(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n","            .csv(air_quality_path)\n","            .select(\n","                col(\"Indicator ID\"),\n","                col(\"name\").alias(\"pollutants\"),\n","                col(\"measure\"),\n","                col(\"Geo Type Name\"),\n","                col(\"Geo Join ID\"),\n","                col(\"Geo Place Name\"),\n","                col(\"Start_Date\")\n","            )\n","            # Convert \"start_date\" to a Spark date\n","            .withColumn(\"start_date\", to_date(col(\"start_date\"), \"yyyy/MM/dd\"))\n","            # Filter date between 2017 and 2019\n","            .filter(col(\"start_date\").between(\"2017-01-01\", \"2019-12-31\"))\n","            # Retain only records for UHF 42\n","            .filter(col(\"Geo Type Name\") == \"UHF 42\")\n","    )\n","\n","    # Map UHF zone numbers to broader district names\n","    air_quality = air_quality.withColumn(\n","        \"zone\",\n","        when(col(\"Geo Join ID\").isin([308, 309, 310]), \"lower-manhattan\")\n","        .when(col(\"Geo Join ID\").isin([306, 307]), \"mid-manhattan\")\n","        .when(col(\"Geo Join ID\").isin([301, 302, 303, 304, 305]), \"upper-manhattan\")\n","        .when(col(\"Geo Join ID\").isin([201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]), \"Brooklyn\")\n","        .when(col(\"Geo Join ID\").isin([101, 102, 103, 104, 105, 106, 107]), \"Bronx\")\n","        .when(col(\"Geo Join ID\").isin([401, 402, 403, 404, 405, 406, 407, 408, 409, 410]), \"Queens\")\n","        .when(col(\"Geo Join ID\").isin([501, 502, 503, 504]), \"Staten Island\")\n","        .otherwise(\"Unknown\")\n","    )\n","\n","    ######################################################\n","    # Join rides data with air quality data on zone + date\n","    ######################################################\n","\n","    # 1) Create a date column for rides\n","    rides = rides.withColumn(\n","        \"trip_start_date\", date_format(col(\"trip_start_timestamp\"), \"yyyy-MM-dd\")\n","    )\n","\n","    # 2) Create a date column for Air Quality\n","    air_quality = air_quality.withColumn(\n","        \"start_date_formatted\", date_format(col(\"start_date\"), \"yyyy-MM-dd\")\n","    )\n","\n","    # 3) Join on (zone + date)\n","    joined_df = (\n","        rides.join(\n","            air_quality,\n","            (rides.zone == air_quality.zone) &\n","            (rides.trip_start_date == air_quality.start_date_formatted),\n","            \"left\"\n","        )\n","        .select(\n","            rides[\"*\"],\n","            air_quality[\"pollutants\"],\n","            air_quality[\"measure\"],\n","            air_quality[\"Geo Join ID\"],\n","            air_quality[\"Geo Place Name\"]\n","        )\n","    )\n","\n","    # 4) Aggregate measure to remove duplicates\n","    final = (\n","        joined_df\n","        .groupBy(\"trip_start_time\", \"trip_start_date\", \"zone\", \"pollutants\")\n","        .agg(\n","            avg(\"measure\").alias(\"measure\"),\n","            first(\"Geo Join ID\").alias(\"geo_join_id\"),\n","            first(\"Geo Place Name\").alias(\"geo_place_name\")\n","        )\n","    )\n","\n","    # Write the final joined data to parquet\n","    # Make sure 'output_path' points to a directory you can actually write to!\n","    final.write \\\n","        .option(\"maxRecordsPerFile\", 100000) \\\n","        .mode(\"overwrite\") \\\n","        .parquet(output_path)\n","\n","    spark.stop()\n","\n","\n","if __name__ == \"__main__\":\n","    main()\n"]}]}