# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VNsVhBHsj4jWVaq8e3hriCUwJqaz8sY4
"""

import sys

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, to_timestamp, date_trunc, row_number, split,
    regexp_extract, regexp_replace, when,
    monotonically_increasing_id, broadcast, date_format, to_date,
    avg, first
)
from pyspark.sql import Window

def main():
    # Expecting three parameters:
    # 1. Citibike rides CSV file paths (comma-separated)
    # 2. Air quality CSV file path
    # 3. Output directory path for the final joined data
    if len(sys.argv) != 4:
        print("Usage: spark-submit revised_spark.py [citibike_paths (comma-separated)] [air_quality_path] [output_path]")
        sys.exit(1)

    # Parse command-line arguments
    citibike_paths = sys.argv[1].split(",")
    air_quality_path = sys.argv[2]
    output_path = sys.argv[3]

    # Create a Spark session
    spark = SparkSession.builder.getOrCreate()

    ######################################################
    # Load and preprocess Citibike rides data
    ######################################################
    rides = (
        spark.read
            .option("header", "true")
            .option("recursiveFileLookup", "true")
            .option("spark.sql.files.maxPartitionBytes", "128MB")
            .csv(citibike_paths)
            .select(
                col("tripduration").alias("trip_duration"),
                col("starttime").alias("trip_start_time"),
                col("startstationid").alias("start_station_id"),
                col("startstationlatitude").alias("start_station_latitude"),
                col("startstationlongitude").alias("start_station_longitude"),
                col("endstationid").alias("end_station_id"),
                col("usertype").alias("user_type")
            )
    )

    # Determine borough based on starting station coordinates.
    rides = rides.withColumn(
        "borough",
        when(
            (col("start_station_latitude").between(40.70, 40.75)) &
            (col("start_station_longitude").between(-74.02, -73.93)),
            "Downtown Manhattan"
        ).when(
            (col("start_station_latitude").between(40.75, 40.82)) &
            (col("start_station_longitude").between(-74.02, -73.93)),
            "Midtown Manhattan"
        ).when(
            (col("start_station_latitude").between(40.82, 40.88)) &
            (col("start_station_longitude").between(-74.02, -73.93)),
            "Uptown Manhattan"
        ).when(
            (col("start_station_latitude").between(40.57, 40.70)) &
            (col("start_station_longitude").between(-74.05, -73.85)),
            "Brooklyn"
        ).when(
            (col("start_station_latitude").between(40.70, 40.80)) &
            (col("start_station_longitude").between(-73.93, -73.70)),
            "Queens"
        ).when(
            (col("start_station_latitude") >= 40.88) & (col("start_station_latitude") < 41.00) &
            (col("start_station_longitude").between(-73.93, -73.70)),
            "The Bronx"
        ).otherwise("Other")
    )

    # Standardize borough names to "zone" for consistency with air quality data.
    rides = rides.withColumn(
        "zone",
        when(col("borough") == "Downtown Manhattan", "lower-manhattan")
        .when(col("borough") == "Midtown Manhattan", "mid-manhattan")
        .when(col("borough") == "Uptown Manhattan", "upper-manhattan")
        .when(col("borough") == "Brooklyn", "Brooklyn")
        .when(col("borough") == "Queens", "Queens")
        .when(col("borough") == "The Bronx", "Bronx")
        .otherwise("Other")
    )

    # Convert trip_start_time to timestamp for further processing.
    rides = rides.withColumn(
        "trip_start_timestamp",
        to_timestamp(col("trip_start_time"), "MM/dd/yyyy HH:mm:ss")
    )

    # (Optional) Write the processed rides data for debugging or separate usage.
    rides.write \
        .option("maxRecordsPerFile", 100000) \
        .mode("overwrite") \
        .parquet("../output/citibike_zones")

    ######################################################
    # Load and preprocess Air Quality data
    ######################################################
    air_quality = (
        spark.read
            .option("header", "true")
            .option("recursiveFileLookup", "true")
            .option("spark.sql.files.maxPartitionBytes", "128MB")
            .csv(air_quality_path)
            .select(
                col("indicator_id"),
                col("name").alias("pollutants"),
                col("measure"),
                col("geo_type_name"),
                col("geo_join_id"),
                col("geo_place_name"),
                col("start_date")
            )
            # Convert "start_date" to a Spark date
            .withColumn("start_date", to_date(col("start_date"), "yyyy/MM/dd"))
            # Filter date between 2017 and 2019
            .filter(col("start_date").between("2017-01-01", "2019-12-31"))
            # Retain only records for UHF 42
            .filter(col("geo_type_name") == "UHF 42")
    )

    # Map UHF zone numbers to broader district names
    air_quality = air_quality.withColumn(
        "zone",
        when(col("geo_join_id").isin([308, 309, 310]), "lower-manhattan")
        .when(col("geo_join_id").isin([306, 307]), "mid-manhattan")
        .when(col("geo_join_id").isin([301, 302, 303, 304, 305]), "upper-manhattan")
        .when(col("geo_join_id").isin([201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]), "Brooklyn")
        .when(col("geo_join_id").isin([101, 102, 103, 104, 105, 106, 107]), "Bronx")
        .when(col("geo_join_id").isin([401, 402, 403, 404, 405, 406, 407, 408, 409, 410]), "Queens")
        .when(col("geo_join_id").isin([501, 502, 503, 504]), "Staten Island")
        .otherwise("Unknown")
    )

    ######################################################
    # Join rides data with air quality data on zone + date
    ######################################################

    # 1) Create a date column for rides
    rides = rides.withColumn(
        "trip_start_date", date_format(col("trip_start_timestamp"), "yyyy-MM-dd")
    )

    # 2) Create a date column for air_quality
    air_quality = air_quality.withColumn(
        "start_date_formatted", date_format(col("start_date"), "yyyy-MM-dd")
    )

    # 3) Join rides and air quality on (zone + date)
    joined_df = (
        rides.join(
            air_quality,
            (rides.zone == air_quality.zone) &
            (rides.trip_start_date == air_quality.start_date_formatted),
            "left"
        )
        .select(
            rides["*"],
            air_quality["pollutants"],
            air_quality["measure"],
            air_quality["geo_join_id"],
            air_quality["geo_place_name"]
        )
    )

    # 4) Aggregate measure to remove duplicates
    final = (
        joined_df
        .groupBy("trip_start_time", "trip_start_date", "zone", "pollutants")
        .agg(
            avg("measure").alias("measure"),
            first("geo_join_id").alias("geo_join_id"),
            first("geo_place_name").alias("geo_place_name")
        )
    )

    # Write the final joined data to parquet
    final.write \
        .option("maxRecordsPerFile", 100000) \
        .mode("overwrite") \
        .parquet(output_path)

    # Stop the Spark session
    spark.stop()

if __name__ == "__main__":
    main()