{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"O3hqD6fcLz4v","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"error","timestamp":1741641023521,"user_tz":420,"elapsed":641,"user":{"displayName":"HAORUI GAO","userId":"04940035010516353456"}},"outputId":"42451aec-ec19-4c62-c0a4-cc39c954f24b"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'rides' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-a5fd3dfa590c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;31m#新增，进行时间转换，用于匹配rides和air quality的年月日\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m rides = rides.withColumn(\n\u001b[0m\u001b[1;32m    149\u001b[0m     \u001b[0;34m\"trip_start_date\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"trip_start_timestamp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yyyy-MM-dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m )\n","\u001b[0;31mNameError\u001b[0m: name 'rides' is not defined"]}],"source":["import sys\n","\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import (\n","    col, to_timestamp, date_trunc, row_number, split,\n","    regexp_extract, regexp_replace, when,\n","    monotonically_increasing_id, broadcast\n",")\n","from pyspark.sql import functions as F\n","from pyspark.sql import Window\n","\n","def main():\n","    # Expecting three parameters:\n","    # 1. Citibike rides CSV file paths\n","    # 2. Air quality CSV file path\n","    # 3. Output directory path for the final joined data\n","    if len(sys.argv) != 4:\n","        print(\"Usage: spark-submit spark-job.py [citibike_paths (comma-separated)] [air_quality_path] [output_path]\")\n","        sys.exit(1)\n","\n","    # Parse command-line arguments\n","    citibike_paths = sys.argv[1].split(\",\")\n","    air_quality_path = sys.argv[2]\n","    output_path = sys.argv[3]\n","\n","    spark = SparkSession.builder.getOrCreate()\n","\n","    ######################################################\n","    # Load and preprocess Citibike rides data\n","    ######################################################\n","    rides = (\n","        spark.read\n","             .option(\"header\", \"true\")\n","             .option(\"recursiveFileLookup\", \"true\")\n","             .option(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n","             .csv(citibike_paths)\n","             .select(\n","                 col(\"tripduration\").alias(\"trip_duration\"),\n","                 col(\"starttime\").alias(\"trip_start_time\"),\n","                 col(\"startstationid\").alias(\"start_station_id\"),\n","                 col(\"startstationlatitude\").alias(\"start_station_latitude\"),\n","                 col(\"startstationlongitude\").alias(\"start_station_longitude\"),\n","                 col(\"endstationid\").alias(\"end_station_id\"),\n","                 col(\"usertype\").alias(\"user_type\")\n","             )\n","    )\n","\n","    # Determine borough based on starting station coordinates.\n","    rides = rides.withColumn(\n","        \"borough\",\n","        when(\n","            (col(\"start_station_latitude\").between(40.70, 40.75)) &\n","            (col(\"start_station_longitude\").between(-74.02, -73.93)),\n","            \"Downtown Manhattan\"\n","        ).when(\n","            (col(\"start_station_latitude\").between(40.75, 40.82)) &\n","            (col(\"start_station_longitude\").between(-74.02, -73.93)),\n","            \"Midtown Manhattan\"\n","        ).when(\n","            (col(\"start_station_latitude\").between(40.82, 40.88)) &\n","            (col(\"start_station_longitude\").between(-74.02, -73.93)),\n","            \"Uptown Manhattan\"\n","        ).when(\n","            (col(\"start_station_latitude\").between(40.57, 40.70)) &\n","            (col(\"start_station_longitude\").between(-74.05, -73.85)),\n","            \"Brooklyn\"\n","        ).when(\n","            (col(\"start_station_latitude\").between(40.70, 40.80)) &\n","            (col(\"start_station_longitude\").between(-73.93, -73.70)),\n","            \"Queens\"\n","        ).when(\n","            (col(\"start_station_latitude\") >= 40.88) & (col(\"start_station_latitude\") < 41.00) &\n","            (col(\"start_station_longitude\").between(-73.93, -73.70)),\n","            \"The Bronx\"\n","        ).otherwise(\"Other\")\n","    )\n","\n","    # Standardize borough names to \"zone\" for consistency with air quality data.\n","    rides = rides.withColumn(\n","        \"zone\",\n","        when(col(\"borough\") == \"Downtown Manhattan\", \"lower-manhattan\")\n","        .when(col(\"borough\") == \"Midtown Manhattan\", \"mid-manhattan\")\n","        .when(col(\"borough\") == \"Uptown Manhattan\", \"upper-manhattan\")\n","        .when(col(\"borough\") == \"Brooklyn\", \"Brooklyn\")\n","        .when(col(\"borough\") == \"Queens\", \"Queens\")\n","        .when(col(\"borough\") == \"The Bronx\", \"Bronx\")\n","        .otherwise(\"Other\")\n","    )\n","\n","    # Convert trip_start_time to timestamp for potential further processing.\n","    rides = rides.withColumn(\n","        \"trip_start_timestamp\", to_timestamp(col(\"trip_start_time\"), \"MM/dd/yyyy HH:mm:ss\")\n","    )\n","\n","    # Write the processed rides data (with zones) for separate usage or debugging.\n","    rides.write \\\n","        .option(\"maxRecordsPerFile\", 100000) \\\n","        .mode(\"overwrite\") \\\n","        .parquet(\"../output/citibike_zones\")\n","\n","    ######################################################\n","    # Load and preprocess Air Quality data\n","    ######################################################\n","    air_quality = (\n","        spark.read\n","            .option(\"header\", \"true\")\n","            .option(\"recursiveFileLookup\", \"true\")\n","            .option(\"spark.sql.files.maxPartitionBytes\", \"128MB\")\n","            .csv(air_quality_path)\n","            .select(\n","                col(\"indicator_id\"),\n","                col(\"name\").alias(\"pollutants\"),\n","                col(\"measure\"),\n","                col(\"geo_type_name\"),\n","                col(\"geo_join_id\"),\n","                col(\"geo_place_name\"),\n","                col(\"start_date\"),\n","            )\n","            .withColumn(\"start_date\", to_date(col(\"start_date\"), \"yyyy/MM/dd\"))\n","            .filter(col(\"start_date\").between(\n","                F.to_date(F.lit(\"2017/01/01\"), \"yyyy/MM/dd\"),\n","                F.to_date(F.lit(\"2019/12/31\"), \"yyyy/MM/dd\")\n","            ))\n","            # Retain only records corresponding to UHF 42\n","            .filter(col(\"geo_type_name\") == \"UHF 42\")\n","    )\n","\n","    # Map UHF zone numbers to broader district names\n","    air_quality = air_quality.withColumn(\n","        \"zone\",\n","        F.when(col(\"geo_join_id\").isin([308, 309, 310]), \"lower-manhattan\")\n","         .when(col(\"geo_join_id\").isin([306, 307]), \"mid-manhattan\")\n","         .when(col(\"geo_join_id\").isin([301, 302, 303, 304, 305]), \"upper-manhattan\")\n","         .when(col(\"geo_join_id\").isin([201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211]), \"Brooklyn\")\n","         .when(col(\"geo_join_id\").isin([101, 102, 103, 104, 105, 106, 107]), \"Bronx\")\n","         .when(col(\"geo_join_id\").isin([401, 402, 403, 404, 405, 406, 407, 408, 409, 410]), \"Queens\")\n","         .when(col(\"geo_join_id\").isin([501, 502, 503, 504]), \"Staten Island\")\n","         .otherwise(\"Unknown\")\n","    )\n","\n","    ######################################################\n","    # Join rides data with air quality data on zone only\n","    ######################################################\n","\n","from pyspark.sql import functions as F #新增，添加计算函数用\n","\n","#新增，进行时间转换，用于匹配rides和air quality的年月日\n","rides = rides.withColumn(\n","    \"trip_start_date\", F.date_format(F.col(\"trip_start_timestamp\"), \"yyyy-MM-dd\")\n",")\n","air_quality = air_quality.withColumn(\n","    \"start_date_formatted\", F.date_format(F.to_date(F.col(\"start_date\"), \"M/d/yyyy\"), \"yyyy-MM-dd\")\n",")\n","\n","#Join rides data with air quality data on zone and time\n","final = rides.join(\n","    air_quality,\n","    (rides.zone == air_quality.zone) & (rides.trip_start_date == air_quality.start_date_formatted), #新增，年月日匹配\n","    \"left\"\n",").select(\n","    rides[\"*\"],\n","    air_quality[\"pollutants\"],\n","    air_quality[\"measure\"],\n","    #air_quality[\"time_period\"],  **去除time_period\n","    air_quality[\"geo_join_id\"],\n","    air_quality[\"geo_place_name\"]\n",")\n","\n","#新增，取平均值Average the measure value to remove duplicates\n","final = final.groupBy(\"trip_start_time\", \"trip_start_date\", \"zone\", \"pollutants\") \\\n","    .agg(F.avg(\"measure\").alias(\"measure\"),  #计算measure平均值\n","         F.first(\"geo_join_id\").alias(\"geo_join_id\"),\n","         F.first(\"geo_place_name\").alias(\"geo_place_name\")\n","    )\n","\n","    # Write final joined data to parquet, limiting each file to 100,000 records.\n","final.write \\\n","    .option(\"maxRecordsPerFile\", 100000) \\\n","    .mode(\"overwrite\") \\\n","    .parquet(output_path)\n","\n","if __name__ == \"__main__\":\n","    main()"]}]}